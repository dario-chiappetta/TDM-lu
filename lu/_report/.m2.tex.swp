\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsmath}

\baselineskip=10pt
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0pt}


\title{\textbf{An efficient comparison algorithm for natural language sentences}}
\author{Dario Chiappetta}


\begin{document}

\maketitle

\begin{abstract}
Computing a \textbf{similarity score} between two natural language \textbf{sentences} is not a trivial problem. Many features can be taken into account to solve it, such as the syntactic structure of the sentence, or the meaning of the single words, or their order; all of this features are informative respect to the meaning of the sentence, and it is reasonable to assume they are all considered by humans solving the same task. Here I describe a recursive, dynamic programming \textbf{algorithm} for sentence comparison aimed to exploit word-based information, as well as the syntactic one, expressed in the form of unsupervised learned binary trees.
\end{abstract}

\section{Introduction}
In the context of sentences \textbf{classification}, where the task is to label an unknown input sentence with the correct meaning, and each meaning is defined by a set of representative natural language sentences, one crucial point is to measure the similarity of the input sentence with each of the sentences defining each meaning.

The problem of scoring the similarity between two sentences is not new in the \textbf{literature}, and a number of different approaches already exist to tackle it. Achananuparp et al. in \cite{achananuparp08} classify these approaches in \emph{word overlap} measures (based on the number of words shared by the two sentences), \emph{TF-IDF} measures (based on term frequency-inverse document frequency) and \emph{linguistic} measures (based on semantic relations between words and their syntactic composition).

Another source of inspiration for this work is represented by \textbf{statistic}, corpus-based methods in Computational Linguistics; a significant example comes from The IBM models for Statistical \textbf{Machine Translation} \cite{ibmmt}, that first introduced the idea of feeding statistically intensive \textbf{Machine Learning} algorithms with big data from corpora, which nowadays is the dominant paradigm in MT; insightful is also the work on Data Oriented Parsing, and particularly the U-DOP model for \textbf{Unsupervised Language Learning} \cite{udop}, which core idea is to initially assume all the possible syntax trees for a set of sentences as equally possible, and then use all the possible sub-trees of them to compute the most probable parse trees, letting the structure of the language emerge from the data.

% A bit more on literature, relation to the work would be nice..

The algorithm presented in this paper aims to provide a similarity measure for two sentences $s_1$ and $s_2$ featuring many of the ideas that have proven to be successful in recent developments of Computational Linguistics. The following are some of the key insights of the algorithm:

\begin{itemize}
\item \textbf{Every possible} sub-sentence of $s_1$ is compared with every possible sub-sentence of $s_2$.
\item A score between two chunks of text is build \textbf{recursively} on the best matches of smaller chunks, and always brought down to scores between single words.
\item \textbf{Dynamic programming} is used to increase the efficiency of the algorithm, preventing it from computing the same result two times.
\item Other than the similarity score, the algorithm outputs the most probable chunking of each sentence in the form of an unlabeled binary \textbf{parse tree}.
\item Other than the similarity score and the parse trees, the algorithm outputs the most likely \textbf{alignment} between the chunks of the two sentences.
\item Every score is expressed as a linear combination of \textbf{features}, designed to be independent and extensible.
\item The partial computations of the algorithm can be stored and further used to implement a learning model.
\end{itemize}

%Document structure
This paper is \textbf{structured} as follows. Section \ref{algo} describes the algorithm theoretically. %and in a better world a complexity analysis
Section \ref{example} walks through a toy example. Section \ref{learning} describes possible extensions and improvements. Section \ref{conclusions} draws the conclusion of the paper.

\section{The algorithm} \label{algo}
\begin{algorithm}
  \SetKwData{Left}{left}
  \SetKwData{Up}{up}
  \SetKwFunction{WWScore}{$\tau$}
  \SetKwFunction{CCScore}{$\tilde{\sigma}$}
  \SetKwFunction{Length}{Length}
  \SetKwFunction{Split}{Split}
  \SetKwFunction{Set}{Set}
  \SetKwFunction{Append}{Append}
  \SetKwFunction{Max}{Max}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKw{In}{in}

\Indm  
  \Input{$c_1,c_2$ chunks of text; $T$ partial results table}
  \Output{$\sigma(c_1,c_2)$ Similarity score between $c_1$ e $c_2$}
\Indp
  \BlankLine
  \uIf{c1,c2 are words}{ \label{main:bc1}
	  \Return{\WWScore{$c_1,c_2$} \label{main:bc2} }
  }
  \BlankLine
  $S\leftarrow[\ ]$\; \label{main:cinit}
  \For{$i\leftarrow 1$ \KwTo \Length{$c_1$}}{ \label{main:loopi}
    $C_1 \leftarrow $ \Split{$c_1$,i}\; \label{main:spliti}
		\For{$j\leftarrow 1$ \KwTo \Length{$c_2$}}{ \label{main:loopj}
		\lIf{i=\Length{$c_1$} and j=\Length{$c_2$}}{break} \label{main:break}
		\BlankLine
		$C_2 \leftarrow $ \Split{$c_2$,j}\; \label{main:splitj}
		\BlankLine
		\ForEach{$sc_1$ \In $C_1$}{ \label{main:fe1}
			\ForEach{$sc_2$ \In $C_2$}{ \label{main:fe2}
				\uIf{$T[sc_1,sc_2]=\emptyset$}{$T[sc_1,sc_2] \leftarrow \sigma(sc_1,sc_2,T)$ \label{main:tabass} \label{main:tupdate}}
			}
		}
		\BlankLine
		\Append{$S$,\CCScore{$C_1,C_2,T$}} \label{main:cappend}
 }
	}
	\BlankLine
	\Return \Max{$R$} \label{main:return}
\caption{The main algorithm\label{main}}
\end{algorithm}

Algorithm \ref{main} contains the main loop of the sentence scoring algorithm. Before entering the details of it, it is necessary to say that:
\begin{itemize}
\item The input $c_1,c_2$ can be any \textbf{chunks} of text because, while the algorithm is called with two sentences in input, it will recursively call itself on every possible chunk extracted from the two sentences.
\item The input $T$ (T for Table) is a data structure holding the intermediate results of the algorithm. As we can infer from line \ref{main:tabass} of Algorithm \ref{main}, $T[c_1,c_2]=\sigma(c_1,c_2)$
\item $\tau(c_1,c_2)$ returns a similarity score between two words. This score is a linear combination of independent features. At the moment, the following features are taken into account:
\begin{itemize}
\item Equality (boolean)
\item Edit distance
\item Difference in position
\item Wordnet Path Length similarity
\end{itemize}
The weights of the single features, now static, are meant to be trained through Machine Learning
\item \texttt{Length($c$)} returns the number of words contained in the chunk $c$
\item \texttt{Split($c,i$)} returns a list of two chunks, one containing the words of $c$ from the first to the $i$th, and the other containing the words of $c$ form the $(i+1)$th to the last.

If the $i$th word is the last word of $c$, it returns a list containing the only element $c$.
\item \texttt{Append($L,x$)} is the standard \emph{append} operation for lists, adding the element $x$ in the last position of the list $L$.
\item $\hat{\sigma}(C_1,C_2,T)$ returns the score of $c_1,c_2$ \textbf{given} a particular 2-split of the two chunks. Such a score is computed combining the ones of the smaller constituents of $c_1,c_2$, which are required to be already present in the table T.

The score is again the linear combination of independent features, leaving its implementation open and extensible.

At the moment the only feature being employed is the average of the scores of the best alignments of the sub-chunks in $C_1$ with the ones in $C_2$.

For example, given $C_1=[\text{"increase","the volume"}]$ and $C_2=[\text{"raise"},\\ \text{"the volume"}]$, "increase" is likely to achieve the best score with "raise" (eg. $T[\text{"increase","raise"}]=0.8$), as well as "the volume" will be aligned with "the volume" ($T[\text{"the volume","the volume"}]=1$)). The score of "increase the volume" and "raise the volume", in this particular splitting, will thus be the average of the two, 0.9.
\end{itemize}
After these premises it is relatively easy to walk through the pseudo-code of Algorithm \ref{main}. Lines \ref{main:bc1} and \ref{main:bc2} handle the base case, that is, when the algorithm is run on \textbf{two words}; in this case the result of the Word Similarity function, which is described above, is returned.

At line \ref{main:cinit} the list of \textbf{candidate results} is initialized as an empty list. This is because more than one score will be computed for $c_1$ and $c_2$, the maximum of which will be returned as a result.

Lines \ref{main:loopi} and \ref{main:loopj} define a nested loop structure, which purpose is to update the two indexes $i$ (from the first to the last word of $c_1$) and $j$ (from the first to the last word of $c_2$) to cover \textbf{every possible combination} of word positions in the two input strings. These indexes are used to \textbf{split} the input chunks in smaller parts (lines \ref{main:spliti} and \ref{main:splitj}). As an example, if the input is given by $c_1=$"increase the volume", $c_2=$"raise the volume", the outer loop will produce the three possible values of $C_1$: ["increase","the volume"], ["increase the","volume"] and ["increase the volume"]; each of these values will be combined, in the inner loop, with the three possible values of $C_2$: ["raise","the volume"], ["raise the","volume"] and ["raise the volume"].

Note that, while it is reasonable to consider the entire $c_1$ and $c_2$ in the comparisons (eg. if $c_1=$"quit",$c_2=$"quit please", we'd want the whole $c_1$ to be associated with the "quit" sub-part of $c_2$), it is necessary to prevent the two entire $c_1$ and $c_2$ to be associated at the same time, thus producing an \textbf{infinite loop}; this is done in line \ref{main:break}.

The next part of the inner loop, from line \ref{main:fe1} to line \ref{main:fe2} combines every sub-chunk of $c_1$ ($sc_1 \in C_1$) with every sub-chunk of $c_2$ ($sc_2 \in C_2$); note that $C_1$ and $C_2$ contain either one or two elements. Every combination that is not present in the table is scored with a \textbf{recursive} call to the algorithm itself, and the resulting score is saved in the table (line \ref{main:tupdate}).

The last operation done in the loop, at line \ref{main:cappend}, is to combine the sub-scores that have just been scored in the table to compute the final score of this particular subdivision of $c_1$ and $c_2$. This is a \textbf{candidate score} for $c_1$ and $c_2$, and thus is appended to the list of candidate scores. As we already said, the definition of this combination is open; however it is most reasonable for the $\hat{\sigma}$ function to find the \textbf{best alignment} between the input sub-chunks, and provide a score based on it. It is plain that the definition of $\hat{\sigma}$ is crucial for getting sensible results from the algorithm. Furthermore, $\hat{\sigma}$ is the point where the algorithm can be extended with Machine Learning capabilities, that will be discussed in Section \ref{learning}.

At line \ref{main:return} the function returns the \textbf{maximum of the candidate} score, that is, the split that produced the best score (eg. for "increase the volume" vs "raise the volume", the score of "increase $\vert$ the volume" vs "raise $\vert$ the volume" is likely to produce a better score than "increase $\vert$ the volume" vs. "raise the $\vert$ volume"). Note that this decision determines the branching of one level of the recursion tree; once the algorithm is done computing the score between two sentences, a traceback of the selected splits can be interpreted as a parse tree for the two input sentences.
\section{An example} \label{example}

\section{Machine Learning Possibilities} \label{learning}

\section{conclusions} \label{conclusions}
Here I described an algorithm to compare the similarity of two input sentences. Its main \textbf{advantages} are the concurrent consideration of many features like word meanings, position and overlapping, as well as the structure of the sentences, having the result in the form of aligned binary trees. The algorithm is recursive and exhaustive in that considers all the possible alignments of all the possible combinations of all the possible binary trees of the two input sentences, maximizing the efficiency with dynamic programming. Due to its modular structure, the algorithm can be easily extended with new features, allowing its integration with well-known Machine Learning techniques in Computational Linguistics (eg. Expectation-Maximization for computing word/phrase alignment probabilities).

There are some assumptions and \textbf{weak points} as well. First of all, the algorithm is inherently binary, in that every recursion step is made of a 2-split of the two input strings; this may have an effect in the way partial scores are combined. More generally, this "two times binary" recursive structure makes the features design task hard, since the effect of one operation is not easy to control over a potentially unlimited number of recursion steps. Also the number of parameters can become high, since each score is a linear combination of single features, requiring a separate Machine Learning training.

Lastly, this paper only describes the \textbf{backbone} of the algorithm, which is not yet ready for more formal tests on existing corpora. The main steps ahead before expecting good results from it include a better tuning and possible extension of the existing features  and the implementation of Machine Learning techniques to include alignment probabilities and more advanced statistical features for the chunks being compared (eg. their degree of informativeness).

\begin{thebibliography}{9}

\bibitem{achananuparp08}
Palakorn Achananuparp, Xiaohua Hu, Xiajiong Shen. The Evaluation of Sentence Similarity Measures. Lecture Notes in Computer Science Volume 5182, 2008, pp 305-316

\bibitem{udop}
Rens Bod. Unsupervised parsing with U-DOP. CoNLL-X '06 Proceedings of the Tenth Conference on Computational Natural Language Learning, 2006, pp 85-92. 

\bibitem{ibmmt}
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer (1993). The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2), 263-311.

\end{thebibliography}

\end{document}
