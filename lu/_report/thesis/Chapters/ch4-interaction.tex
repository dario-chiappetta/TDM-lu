% Chapter Template

\chapter{Interaction} % Main chapter title

\label{ch:interaction} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\lhead{Chapter \ref{ch:interaction}. \emph{Interaction}} % Change X to a consecutive number; this is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	INTRO TEXT
%----------------------------------------------------------------------------------------
In the previous chapter we have seen how the Language Unit is able to associate utterances from the user with their most likely meaning; due to the nature of the problem, it cannot be guaranteed that the output meaning is actually the one intended by the user. However, we have also seen that such an association is the result of a fuzzy matching process, where the utterance receives a comparison score with each of the candidate meanings. When these scores are accurate, they provide an important \textbf{estimate of the confidence} whether the understanding is correct or not.

This chapter describes how the Language Unit exploits the information derived from the matching scores, to decide on whether a grounding interaction with the user is needed, and in case asking \textbf{pertinent questions} about the meaning of single sentence components. Also, an explanation of the \textbf{learning} process is given, that is, how the LU makes use of the user answers to update its knowledge about the language.

Section \ref{ch:interaction:mmw} describes how the best interaction policy is selected; Section \ref{ch:interaction:afl} explains how the software locates the minimal unclear constituents in the input sentence; Section \ref{ch:interaction:episode} deals with how the Language Unit interact with TDM to produce the grounding subdialogues; Section \ref{ch:interaction:learn} describes how the user answers are used to learn new information about the language.

%----------------------------------------------------------------------------------------
%	MEANING MATCHING WORKFLOW
%----------------------------------------------------------------------------------------

\section{Meaning Matching Workflow} \label{ch:interaction:mmw}
The first problem the Language Unit has to face, is to determine whether the understanding of an input utterance is good enough to be taken as correct, or it rather needs to be grounded with the user.

\subsection{Overview}
Let's consider the following situation:

\texttt{S> How can I help you? \\
U> what is the title of this song}

At this point, four different scenarios are possible:
\begin{enumerate}
	\item There is one best matching meaning, and its confidence score is high.
	\item There is one best matching meaning, but its confidence score is low.
	\item There are two or more meanings with a high confidence score.
	\item No meaning has a good confidence score.
\end{enumerate}
These different situations can be easily associated with as many \textbf{interaction policies}:
\begin{enumerate}
	\item Accept the best meaning without interaction.
	\item Ask whether the understanding of the best meaning is correct.
	\item Ask the user to disambiguate between the top candidates.
	\item Ask the user to rephrase his utterance.
\end{enumerate}

\subsection{Implementation: the \texttt{get\_plan()} Function} \label{ch:interaction:mmw:getplan}
The work of \textbf{deciding on the policy} to adopt is done by the \texttt{lu.learn.interaction} module, and especially by the \texttt{get\_plan()} function. This function takes a list of candidate meanings (and their confidence scores) as input, and outputs the plan that the dialogue manager should follow. In the case of TDM, the \texttt{get\_plan\_tdm()} function can be called, which returns the plan in the form of TDM-compatible dialogue moves\footnote{Due to the lack of documentation of the TDM code, it was not possible to integrate the grounding moves into the dialogue management library. Instead, a generic entry-point-move is used to start grounding subdialogues that are in fact controlled by the Language Unit. More details will be given in \ref{ch:interaction:episode}.}.

At first, \texttt{get\_plan()} \textbf{normalizes} the input scores, so that they sum up to 1. This allows us to compare them scores more easily\footnote{Note that, even if this is not done in the current implementation, one could think of using the un-normalized values to have an idea of the absolute confidence of the matches.}.

When the scores are normalized, \textbf{confidence thresholds} can be set in order to detect the four cases showed in \ref{ch:interaction:mmw}:
\begin{enumerate}
	\item The best score is greater than a constant $\alpha$. The value of $\alpha$ should be chosen so that $\alpha > 0.5$; in this way it is guaranteed that at most one meaning will be selected with maximum confidence.
	\item There is one and only one meaning whose confidence score is greater than a constant $\beta$, and yet smaller than $\alpha$. The value of $\beta$ should be chosen so that $\beta < \alpha$.
	\item There are two or more meanings whose confidence score is greater than $\beta$ and smaller than $\alpha$. In order to allow for this possibility, $\beta$ should be chosen so that $\beta < 0.5$
	\item None of the confidence scores is greater than $\beta$.
\end{enumerate}

In the experiments, the $\alpha$ and $\beta$ \textbf{constants} have been manually set to, respectively, 0.7 and 0.3. As it will be better discussed in Chapter \ref{ch:conclusions}, a better way to select these parameters would be to employ a meta-learning procedure to train the hypermodel for optimal performance.

At this point, \textbf{cases 1 and 4} are trivial to handle: \texttt{get\_plan()} will return the best matched meaning in the first case, and the meta-goal \texttt{\_rephrase}\footnote{Note that, since \texttt{get\_plan()} is called by TDM to interpret user utterances, formally this means ``User asked System to ask User to rephrase". A more elegant way of solving this issue would be to integrate rephrase requests in TDM, but this, again, was not possible due to lack of documentation.} in the latter. \texttt{\_rephrase} has the effect of terminating the interaction with the system utterance
\enumsentence{I do not quite understand what you mean, can you rephrase please?}

The following dialogue shows \pname dealing with the two situations that have just been described:

\texttt{S> How can I help you? \\
U> what is the current song \\
S> Running too fast, by Rehearsal Summer \\
S> How can I help you? \\
U> I like biscuits! \\
S> I do not quite understand what you mean, can you rephrase please?}

Cases \textbf{2 and 3} require a more sophisticated handling. In this cases, the Language Unit will first try to locate specific parts of the utterance that are particularly unclear (see \ref{ch:interaction:afl}), and then use them to produce a targeted clarification request to ground the correct meaning of the user utterance (see \ref{ch:interaction:episode}).

%----------------------------------------------------------------------------------------
%	MINIMAL AMBIGUOUS FRAGMENT
%----------------------------------------------------------------------------------------

\section{Ambiguous Fragment Location} \label{ch:interaction:afl}
When the Language Unit is not sure about the meaning of the user utterance, a procedure is used to reduce the uncertainty to the \textbf{smallest possible constituents} of the sentence. This is done by analyzing the similarity between the user utterance and the best matching sentence within each of the candidate meanings.

\subsection{Overview}
Let's consider, as an example, the meaning \texttt{ask(?X.current\_song(X))}, defined by the sentences in \ref{ch:interaction:afl:meaning}, and the question in \ref{ch:interaction:afl:whatisthis}. If this question matches this meaning, achieving its best score with the sentence in  \ref{ch:interaction:afl:meaningmatch}, the software should then return ``the current" from \ref{ch:interaction:afl:whatisthis} and ``this" from \ref{ch:interaction:afl:meaningmatch} as the minimal uncertain alignment, because all the other parts are \textbf{perfectly matched}. The way this especially unclear constituents are detected will de described in \ref{ch:interaction:afl:impl}.
\eenumsentence{\item What is the current song \label{ch:interaction:afl:meaningmatch}
			     \item Which song is playing
			     \item Which track is playing} \label{ch:interaction:afl:meaning}
\vspace{-0.5cm} %HACK
\enumsentence{What is this song? \label{ch:interaction:afl:whatisthis}} 
Of course, the algorithm should have a \textbf{margin of tolerance} as to the constituents whose match should be considered accepted. As an example, the sentence in \ref{ch:interaction:afl:whats} shouldn't raise concerns about the ``What's"/``What is" alignment, when compared against \ref{ch:interaction:afl:meaningmatch}; this is because, even though the match between ``What's" and ``What is" is not perfect, it is expected to achieve a sufficiently high score to be considered matched without questions.
\enumsentence{What's this song? \label{ch:interaction:afl:whats}}


\subsection{Implementation: the \texttt{locate\_fragments()} Function} \label{ch:interaction:afl:impl}
Algorithm \ref{ch:interaction:afl:algo} defines the \texttt{locate\_fragments()} function, which is used to locate, in the utterance from the user, the \textbf{constituents} that are especially unclear.

\begin{algorithm}
  \SetKwData{Left}{left}
  \SetKwData{Up}{up}
  \SetKwFunction{WWScore}{$\tau$}
  \SetKwFunction{CCScore}{$\tilde{\sigma}$}
  \SetKwFunction{from}{from}
  \SetKwFunction{to}{to}
  \SetKwFunction{getalignment}{get\_alignment}
  \SetKwFunction{getscore}{get\_score}
  \SetKwFunction{min}{min}
  \SetKwFunction{Length}{Length}
  \SetKwFunction{Split}{Split}
  \SetKwFunction{Set}{Set}
  \SetKwFunction{Append}{Append}
  \SetKwFunction{Max}{Max}
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \SetKw{In}{in}

  \SetKwFunction{algo}{locate\_fragments}
  
  \SetKwProg{myalg}{Procedure}{}{}
  \myalg{\algo{}}{
\lIf{s is WordScore}{\Return{s.\from, s.\to}} \label{ch:interaction:afl:algo:recstep1}
  \BlankLine
  $s_1\leftarrow$ s.\getalignment{$1$}.\getscore{}\; \label{ch:interaction:afl:algo:al1}
  $s_2\leftarrow$ s.\getalignment{$2$}.\getscore{}\; \label{ch:interaction:afl:algo:al2}
  $s_{1_N} \leftarrow \frac{s_1}{s_1+s_2}$\;  \label{ch:interaction:afl:algo:norm1}
  $s_{2_N} \leftarrow \frac{s_2}{s_1+s_2}$\;  \label{ch:interaction:afl:algo:norm2}
  \BlankLine
  \eIf{$|s_{1_N}-s_{2_N}|>\gamma$}{\label{ch:interaction:afl:algo:if}
	\Return{\algo{\min{$s_1,s_2$}}}
  }{
	\Return{s.\from, s.\to}
  } \label{ch:interaction:afl:algo:endif}
  }
\caption{The minimal ambiguous fragment location algorithm\label{ch:interaction:afl:algo}}
\end{algorithm}

\texttt{locate\_fragments()} is a \textbf{recursive} function, that receives a Sentence Score $s$ as input\footnote{Note that, both here and in the actual implementation, a Score is not just a numeric value, but also contains information about the matched elements. For instance, scores define the fields \texttt{from} and \texttt{to}, containing the two elements (sentences, chunks, or words) the score refers to.} (it will be fed with the score between the user utterance and the best matching sentence in every candidate meaning of it), and outputs the two minimal constituents for which the alignment is uncertain (e.g.\ respect to the example mentioned in \ref{ch:interaction:afl}, it is expected to output the two chunks ``the current" and ``this"). This information will be used to produce \textbf{clarification questions} for the user (e.g.\ ``Do you mean `the current' when you say `this'?").

We can see at \textbf{line \ref{ch:interaction:afl:algo:recstep1}} that one reason that ends the recursion is when a \texttt{WordScore} object is given as input. This is clearly because no smaller constituents can be found in a score between two words.

If the input score is not a \texttt{WordScore}, it must then be a \texttt{ChunkScore}\footnote{It could also be a \texttt{SentenceScore}: this happens in the first call to the algorithm. However, in the current implementation, the \texttt{SentenceScore} class is just an alias for \texttt{ChunkScore} (see \ref{ch:arch:LU:scores:sentence})}. We know from Chapter \ref{ch:M2} that every score between two chunks builds on two scores of smaller chunks; for instance, the score between sentences \ref{ch:interaction:afl:whatisthis} and \ref{ch:interaction:afl:meaningmatch}, $\sigma_S$(``What's this song",``What is the current song"), will ideally contain the scores $\sigma_C$(``what's",``what is") and $\sigma_C$(``the current song",``this song"). \textbf{Lines \ref{ch:interaction:afl:algo:al1} and \ref{ch:interaction:afl:algo:al2}} retrieve the scores of these two smaller alignments from the main score object.

\textbf{Lines \ref{ch:interaction:afl:algo:norm1} and \ref{ch:interaction:afl:algo:norm2}} normalize the inner alignment scores so that they sum up to 1. This is to uniform the values the algorithm will deal with in the next steps.

The final \texttt{if} construct (\textbf{lines \ref{ch:interaction:afl:algo:if} to \ref{ch:interaction:afl:algo:endif}}) looks at the difference between the scores of the inner alignments: if there is a significant difference between these two scores ($\gamma$ is a free parameter) then repeat the algorithm on the weakest alignment (because it means that it contains the misunderstood part, while the other was understood with good confidence), otherwise return the whole chunks contained in the input score (because it means that both the alignments are somehow weak).

%----------------------------------------------------------------------------------------
%	INTERACTION EPISODE
%----------------------------------------------------------------------------------------

\section{Production of the Interaction Episode} \label{ch:interaction:episode}
Once the need for grounding is enstablished (\ref{ch:interaction:mmw}), and the software has detected the core of the uncertainty (\ref{ch:interaction:afl}), it is necessary to produce a dialogue with the user, to put the clarification plan into practice.

\subsection{Overview}
In \ref{ch:interaction:mmw} we have encountered \textbf{two cases} in which an interaction with the user is necessary to ensure the correctness of the interpretation:
\begin{itemize}
	\item The best matched meaning did not achieved enough confidence to be taken as correct
	\item More than one meaning achieved an equally high level of confidence
\end{itemize}

Even though these two situations could be approached in two different ways, only \textbf{one grounding subdialogue} has been designed. and that is identified by the label \texttt{\_ground}.

This episode, due in part to technical restrictions (as it will be explained in \ref{ch:interaction:episode:impl}), is very simple, and consists of a single question, that can be answered affirmatively or negatively by the user. The following is an example of a \texttt{\_ground} episode:

\texttt{S> How can I help you? \\
U> what is the title of this song \\
S> Does `title of this song' mean `current song'? \\
U> yes \\
S> Thank you for the feedback! \\
S> Young Lust, by Pink Floyd}

Note that the \textbf{clarification question} can be any string, and can make use of the information returned by \texttt{locate\_fragments()}, that is, the two chunk of text whose alignment is uncertain. In the current implementation, this question is always given in the form of ``Does X mean Y", where X and Y are the chunks returned by \texttt{locate\_fragments()} (see \ref{ch:interaction:afl}); different formulations could be thought, to obtain a more natural interaction with the user (e.g.\ simply asking ``You mean `current song'?").

If there is only \textbf{one candidate} meaning (case 2 in \ref{ch:interaction:mmw}), the question will be referred to that meaning, \textbf{otherwise} (case 3 in \ref{ch:interaction:mmw}) the question will be about the meaning that achieved the best score. Note that in fact this mean that the two cases are \textbf{not distinguished} in any way from each other. However, it is also worth to note that, even though only the simplest part of it is implemented in the dialogue manager, the plan produced by the Language Unit does differenciate the two situations, and includes nested questions, which utterance is conditioned on the answer of the top level ones.

\subsection{Implementation} \label{ch:interaction:episode:impl}

As it has already been mentioned, the TDM library comes with no technical documentation. This unfortunately limited us to the design of extremely simple grounding interactions. For the same reason, the design and implementation of such interaction, which is described in this section, is to be considered as a substantial \textbf{workaround}, wich does not fit the design principles of neither TDM or LU: an integral rewrite should be considered upon availability of TDM's documentation.

Every time the \texttt{get\_plan()} function of the \textbf{Language Unit} (see \ref{ch:interaction:mmw:getplan}) encounters one of the situations mentioned in \ref{ch:interaction:episode}, instead of returning an ordinary dialogue move, it performs the following operations: 
\begin{enumerate}
	\item Pushes the uncertain meaning in a stack of open grounding issues. This stack is defined in \texttt{lu.learn.interaction} as \texttt{tdm\_ground\_stack}.
	\item Returns the \texttt{request(\_ground)} meta-move to TDM, that called the LU in the first place. This call is made from \texttt{tdm.lu\_grammar}, by the method \texttt{utterance\_to\_moves} of the class \texttt{LuGrammar}, which is itself called by method \texttt{interpret} of class \texttt{InterpretModule}, defined in \texttt{tdm.interpret}. Along with the grounding request, the LU also sends the English question that will be asked to the user.
\end{enumerate}

The following actions are performed by \textbf{TDM} upon receival of the \texttt{request(\_ground)}:
\begin{enumerate}
	\item Changes the realization string for the meaning \texttt{ask(?X.\_ground\_X\_to\_Y(X))} (which will be later used to ask the question to the user)  into the question provided by the LU.
	\item Executes the normal plan associated with  \texttt{\_ground}.
\end{enumerate}

The \texttt{\_ground} \textbf{plan} is made of two parts:
\begin{enumerate}
	\item \texttt{findout(?X.\_ground\_X\_to\_Y(X))}, where the system question generated by the Language Unit is answered with yes or no.
	\item \texttt{dev\_perform(Ground, MplayDevice)}; the \texttt{Ground} device action is triggered to handle the answer to the previous question.
\end{enumerate}

The \texttt{Ground} \textbf{device} action terminates the interaction with the user. If the answer was negative (X does not mean Y), grounding was not successful and the system simply concludes the interaction asking the user to rephrase its utterance. Otherwise, if the answer was positive, the following grounding operations are performed:
\begin{enumerate}
	\item Contacts the Language Unit to solve the top open grounding issue. This will pop the top element of the stack of open grounding issues, and possibly start the learning process.
	\item Execute the action the user wanted in the first place.
\end{enumerate}

The second point is somehow tricky, in that there is no known way for the device to interfere with the dialogue management operations. The solution to this problem is particularly inelegant: the device imports the Turn Manager, wich has been extended with a \texttt{ground\_hack()} method. This method has the effect of posting the ad-hoc \texttt{GROUND\_HACK} event in the system, with attached the meaning label of the action to run. A handler for this event has been written in the \texttt{tdm.interpret} module, which receives the meaning string, parses it, and finally throws the \texttt{INTERPRETATION} event that will run the action.

%----------------------------------------------------------------------------------------
%	KNOWLEDGE UPDATE
%----------------------------------------------------------------------------------------

\section{Knowledge Update} \label{ch:interaction:learn}

Every time a new labeled example (sentence + meaning label) is encountered, the Language Unit updates its knowledge about the language. In the current implementation, learning a \textbf{new sentence} consists of
\begin{itemize}
	\item Adding the \textbf{new example} to the set of the sentences that realize its meaning
	\item Updating the global \textbf{chunk count} (see \ref{ch3:ml:cl}) for every possible substring of the input sentence
	\item Updating the \textbf{class-conditional chunk count} (see \ref{ch3:ml:cc}) for every possible substring of the input sentence
	\item Updating the \textbf{alignment mass} (see \ref{ch3:ml:al}) for the alignments that are found parsing the new utterance with the ones already existing realizing its meaning.
\end{itemize}
These operations are done by the \texttt{lu.learn.sentence} module, and especially by its \texttt{learn()} method.

Adding the new example to the meaning definition is trivial, and incrementing the counters is straightforward as well: a procedure loops through every possible substring of the new sentence, and increments the corresponding global and class-conditional counter. Nevertheless, it is worth to spend some words on the last one, the \textbf{alignment mass update}. To update this value, the new sentence is scored against all the other sentences in the meaning; as we have already seen, every Score object carries information about the internal alignments of the two sentences (i.e.\ a Score is either a \texttt{WordScore}, or is built upon two smaller scores). What the learning procedure does is looping through this boxed scores, and for each of them adds its numeric value to the alignment mass already occupied by that score.

It is also useful to remember that, at the moment of learning, the user gave us information about the most \textbf{uncertain alignment} in the sentence (see \ref{ch:interaction:afl}), confirming that it is a valid alignment. This information is used to enhance the learning process, because, without the user answer, that alignment would receive a small mass increment, as its score is not expected to be high. But, considering the user input, the alignment can be reinforced by a significant value (e.g.\ 1), increasing the probability of it being recognised correctly in other contexts.

\section{Summary}
In this chapter we have described the way the Language Unit deals with potentially misinterpreted utterances.

The first step of the process is to \textbf{detect} whether the interpretation contains uncertainty or not. This is done by looking at the confidence score of the interpretation; four cases can be identified: no uncertainty, uncertainty with one candidate meaning, uncertainty with two or more candidate meanings, uncertainty with no candidate meanings.

When there is uncertainty and at least one candidate meaning, the software will proceed to \textbf{locate} the uncertainty, that is, finding the most specific possible constituent in the user utterance that caused the misinterpretation. This is done analyzing recursively the components of the score the input sentence achieved with the most similar example in each of the candidate meanings.

The next step will be to start a \textbf{clarification} subdialog with the user, to determine whether the uncertain alignment was legit or not. This dialog is always given in the form of a single modal question.

When the answer to the clarification question is positive, meaning that the software guess on the interpretation was correct, the software \textbf{learns} from the episode, adding the new sentence to the realizations of its meaning, and especially reinforcing the uncertain alignment, in order for it to be recognised in further situations.